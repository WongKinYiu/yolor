{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models import *\n",
    "from utils.datasets import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "cfg = './cfg/yolor_p6.cfg'\n",
    "imgsz = 640\n",
    "weight = \"./weights/best.pt\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = Darknet(cfg, imgsz).cuda()\n",
    "model.load_state_dict(torch.load(weight, map_location=device)['model'])\n",
    "#model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "#imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
    "model.to(device).eval()\n",
    "half = device.type != 'cpu' \n",
    "if half:\n",
    "    print(\"HALF\")\n",
    "    model.half()  # to FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "_ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "        'headline',\n",
    "        'doc',\n",
    "        'cir_stamp',\n",
    "        'rec_stamp'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_illigal_point(point, pmin, pmax):\n",
    "    point = round(point)\n",
    "    if point < pmin:\n",
    "        return pmin\n",
    "    if point > pmax:\n",
    "        return pmax\n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_inference(img_path, model, conf_thres = 0.4, iou_thres=0.5):\n",
    "    with torch.no_grad():\n",
    "        # dataset = LoadImages(source, img_size=imgsz, auto_size=64)\n",
    "        ori_img = cv2.imread(img_path)\n",
    "        ori_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w, c = ori_img.shape\n",
    "        print(ori_img.shape)\n",
    "        img = letterbox(ori_img, new_shape=imgsz, auto_size=64)[0]\n",
    "        # img = letterbox(img0, new_shape=self.img_size, auto_size=self.auto_size)[0]\n",
    "\n",
    "                # Convert\n",
    "        scaled_h, scaled_w, c = img.shape\n",
    "        h_ratio = scaled_h/img_h\n",
    "        w_ratio = scaled_w/img_w\n",
    "        \n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        # t1 = time_synchronized()\n",
    "        pred = model(img)[0]\n",
    "        pred = non_max_suppression(pred, conf_thres=conf_thres, iou_thres=0.5, classes=None) #[[left, top, right, bottom, probs, cls]]\n",
    "        np_pred = pred[0].cpu().detach().numpy()\n",
    "\n",
    "        draw_img = ori_img.copy()\n",
    "        for each_pred in np_pred:\n",
    "            relocate_point = []\n",
    "            left, top, right, bottom, probs, cls = each_pred\n",
    "            left = left/w_ratio\n",
    "            right = right/w_ratio\n",
    "            top = top/h_ratio\n",
    "            bottom = bottom/h_ratio\n",
    "\n",
    "            left = relocate_illigal_point(left, 0, img_w)\n",
    "            top = relocate_illigal_point(top, 0, img_h)\n",
    "            right = relocate_illigal_point(right, 0, img_w)\n",
    "            bottom = relocate_illigal_point(bottom, 0, img_h)\n",
    "\n",
    "            cv2.rectangle(draw_img, (left, top), (right, bottom), (255,0,0), 1)\n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"01F52F8C-1EA0-4DC4-821B-068FF7D30C00_IzCfN_1670037805225.jpg\"\n",
    "draw_img = single_inference(img_path, model, conf_thres=0.5, iou_thres=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image.fromarray(draw_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, det in enumerate(pred):\n",
    "    p, s, im0 = img_path, '', img\n",
    "    s += '%gx%g ' % img.shape[2:]  # print string\n",
    "    gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "    if det is not None and len(det):\n",
    "        # Rescale boxes from img_size to im0 size\n",
    "        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "        # Print results\n",
    "        for c in det[:, -1].unique():\n",
    "            n = (det[:, -1] == c).sum()  # detections per class\n",
    "            s += '%g %ss, ' % (n, names[int(c)])  # add to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_infer(img_path):\n",
    "    # t0 = time.time()\n",
    "    source = \"./output.png\"\n",
    "    dataset = LoadImages(source, img_size=imgsz, auto_size=64)\n",
    "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "    _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        # t1 = time_synchronized()\n",
    "        pred = model(img)[0]\n",
    "        pred = non_max_suppression(pred, conf_thres=0.4, iou_thres=0.5, classes=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3.4e+17\n",
    "'{:17f}'.format(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"340000000000000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import DocumentLayoutDetection\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document detection model loading completed by cpu\n"
     ]
    }
   ],
   "source": [
    "cfg_path = \"./custom_cfg/ppy_yolor_p6_4cls.cfg\"\n",
    "weight_path = \"./weights/best.pt\"\n",
    "DocLayoutDet = DocumentLayoutDetection(cfg_path, weight_path, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.imread(\"a.jpg\")\n",
    "detected_objs = DocLayoutDet(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [{'bbox': [273, 202, 490, 230],\n",
       "               'score': 0.8559931,\n",
       "               'cls_name': 'headline',\n",
       "               'cls_id': 0}],\n",
       "             1: [{'bbox': [0, 84, 798, 662],\n",
       "               'score': 0.79706466,\n",
       "               'cls_name': 'doc',\n",
       "               'cls_id': 1}],\n",
       "             2: [{'bbox': [102, 179, 237, 307],\n",
       "               'score': 0.746423,\n",
       "               'cls_name': 'cir_stamp',\n",
       "               'cls_id': 2}]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_color = [\n",
    "    (252, 200, 154),\n",
    "    (255, 248, 230),\n",
    "    (255, 148, 172),\n",
    "    (80, 92, 153),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_img = test_img.copy()\n",
    "for each_cls, each_objs in detected_objs.items:\n",
    "    for each_obj in each_objs:\n",
    "        left, top, right, bottom = each_obj[\"bbox\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc5644fd2b52d511b211cf140a1e1d228fa8006a12c8d99adabf32e3d6df441d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
